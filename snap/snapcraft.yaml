name: mistral-7b-instruct
base: core24
version: &snap-version "v0.3"
summary: Mistral 7B Instruct
description: |
  This snap installs an optimized environment for inference with the
  mistral-7b-instruct LLM.


grade: devel
confinement: strict

compression: lzo

plugs:
  graphics:
    interface: content
    target: $SNAP/graphics

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc

hooks:
  install:
    plugs:
      # For hardware-info
      - hardware-observe
      - opengl

parts:
  chat:
    source: chat.sh
    source-type: file
    plugin: dump

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/

  hardware-info:
    source: hardware-info
    plugin: go
    build-snaps:
      - go/latest/stable
    stage-packages:
      - pciutils
      - nvidia-utils

  stack-selector:
    source: stack-selector
    plugin: go
    build-snaps:
      - go/latest/stable

  common-runtime-dependencies:
    plugin: nil
    stage-packages:
      - wget # model init
      - jq # install hook
  
  mistral-inference:
    source: components/mistral-inference
    plugin: python
    python-packages:
      - mistral-inference==1.5.0
    organize:
      "*": (component/mistral-inference)

  llamacpp:
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b4130 
    source-depth: 1
    plugin: make
    override-build: |
      make --jobs=$(nproc)
      cp -v llama-cli $CRAFT_PART_INSTALL/
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  model:
    source: components/model
    plugin: dump
    organize:
      "*": (component/model)

  model-q4-k-m-gguf:
    source: components/model-q4-k-m-gguf
    plugin: dump
    organize:
      "*": (component/model-q4-k-m-gguf)

  model-f32-gguf:
    source: components/model-f32-gguf
    plugin: dump
    organize:
      "*": (component/model-f32-gguf)
      
components:
  # engines
  mistral-inference:
    type: test
    summary: Mistral Inference
    description: Inference Engine from Mistral
    version: "1.5.0" # should match the python package
  llamacpp:
    type: test
    summary: llama.cpp Engine
    description: LLM inference in C/C++
    version: *llamacpp-tag
  
  # models
  model:
    type: test
    summary: Mistral 7B Instruct
    description: Original model with safetensors, params, and tokenizer
    version: *snap-version
  model-q4-k-m-gguf:
    type: test
    summary: Mistral 7B Instruct Q4_K_M
    description: Quantized model in gguf format with Q4_K_M weight encoding
    version: *snap-version
  model-f32-gguf:
    type: test
    summary: Mistral 7B Instruct F32
    description: Model in gguf format with F32 weight encoding
    version: *snap-version

apps:
  chat:
    command: chat.sh
    plugs:
      - opengl
      # Needed by mistral-inference,
      # for cuda-uvmfd to listen on a unix socket
      - network-bind
      # Needed to download resources
      - network
    environment:
      # Needed for shared libraries used by llama.cpp's cli
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR
