name: mistral-7b-instruct
base: core24
version: "v0.3+0.0.1"
summary: Mistral 7B Instruct
description: |
  This snap installs an optimized environment for inference with the
  mistral-7b-instruct LLM.


grade: devel
confinement: strict

compression: lzo

environment:
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

parts:
  local:
    source: snap/local
    plugin: dump
  
  mistral-inference:
    source: inference/mistral-inference/engine
    plugin: python
    python-packages:
      - mistral-inference==1.5.0
    organize:
      "*": (component/mistral-inference)
  
  mistral-cli:
    source: inference/mistral-inference/cli
    plugin: dump
    organize:
      "*": (component/mistral-inference)/bin/

  mistral-7b-instruct-model:
    source: mistral-7B-Instruct-v0.3.tar
    plugin: dump
    organize:
      "*": (component/mistral-7b-instruct-model)
    override-build: |
      craftctl default
      chmod 664 $CRAFT_PART_INSTALL/consolidated.safetensors

components:
  mistral-inference:
    type: test
    summary: Mistral Inference
    description: Inference Engine from Mistral
    version: "1.5.0" # should match the python package
  mistral-7b-instruct-model:
    type: test
    summary: Mistral 7B Instruct
    description: Model with safetensors, params, and tokenizer
    version: "v0.3" # should be consistent with the package version

apps:
  chat:
    command: bin/chat.sh
    plugs:
      - opengl
      - network-bind # this is for cuda-uvmfd to listen on a unix socket

plugs:
  graphics:
    interface: content
    target: $SNAP/graphics
