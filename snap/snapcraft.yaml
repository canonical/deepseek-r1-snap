name: deepseek-r1
base: core24
version: &snap-version "v3" # Refers to DeepSeek V3 base
summary: DeepSeek R1
description: |
  This snap installs an optimized environment for inference with the
  DeepSeek R1 LLM.

grade: devel
confinement: strict

compression: lzo

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc

hooks:
  install:
    plugs: &install-plugs
      # For hardware-info
      - hardware-observe
      - opengl

parts:
  app-scripts:
    source: apps
    plugin: dump
    organize:
      "*": bin/

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/

  ml-snap-utils:
    source: ml-snap-utils
    plugin: go
    build-snaps:
      - go/latest/stable
    stage-packages:
      - pciutils
      - nvidia-utils

  common-runtime-dependencies:
    plugin: nil
    stage-packages:
      - wget # model init
      - jq # install hook
    stage-snaps:
      - yq # install hook

  llamacpp: &llamacpp-part
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b4595 
    source-depth: 1
    plugin: cmake
    cmake-parameters:
      - -DGGML_NATIVE=OFF
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  llamacpp-avx512:
    <<: *llamacpp-part
    cmake-parameters:
      - -DGGML_NATIVE=OFF
      - -DGGML_AVX512=ON
    organize:
      "*": (component/llamacpp-avx512)

  llamacpp-cuda:
    <<: *llamacpp-part
    build-packages:
      - nvidia-cuda-toolkit
    cmake-parameters:
      - -DGGML_NATIVE=OFF
      - -DGGML_CUDA=ON
      - -DCMAKE_CUDA_ARCHITECTURES=all-major
    organize:
      "*": (component/llamacpp-cuda)

  model-distill-qwen-1-5b-q8-0-gguf:
    source: components/model-distill-qwen-1-5b-q8-0-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-qwen-1-5b-q8-0-gguf)
  
  model-distill-qwen-7b-q4-k-m-gguf:
    source: components/model-distill-qwen-7b-q4-k-m-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-qwen-7b-q4-k-m-gguf)
  
  model-distill-llama-70b-q4-k-m-gguf:
    source: components/model-distill-llama-70b-q4-k-m-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-llama-70b-q4-k-m-gguf)
      
components:
  # engines
  llamacpp:
    type: test
    summary: llama.cpp Engine
    description: LLM inference in C/C++
    version: *llamacpp-tag
  llamacpp-avx512:
    type: test
    summary: llama.cpp Engine using avx, avx2 and avx512 CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag
  llamacpp-cuda:
    type: test
    summary: llama.cpp Engine for NVIDIA GPUs using CUDA
    description: LLM inference in C/C++
    version: *llamacpp-tag
  
  # models
  model-distill-qwen-1-5b-q8-0-gguf:
    type: test
    summary: DeepSeek R1 Distill Qwen 1.5B Q8
    description: Quantized model with 1.5B parameters in gguf format with Q8_0 weight encoding
    version: *snap-version

  model-distill-qwen-7b-q4-k-m-gguf:
    type: test
    summary: DeepSeek R1 Distill Qwen 7B Q4
    description: Quantized model with 7B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

  model-distill-llama-70b-q4-k-m-gguf:
    type: test
    summary: DeepSeek R1 Llama 70B Q4
    description: Quantized model with 70B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

apps:
  # For testing purposes. Use sudo <snap>.re-install
  # Connecting the hardware-observe interface is required even if installed in dev mode!
  re-install:
    command: meta/hooks/install
    plugs: *install-plugs

  init:
    command: bin/init.sh
    plugs:
      # Needed to download resources
      - network

  chat: &engine-app
    command: bin/chat.sh
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

  server: 
    <<: *engine-app
    command: bin/server.sh
    plugs:
      - network-bind
    daemon: simple
    install-mode: disable
