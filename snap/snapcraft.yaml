name: deepseek-r1
base: core24
version: &snap-version "v3" # Refers to DeepSeek V3 base
summary: DeepSeek R1
description: |
  This snap installs an optimized environment for inference with the
  DeepSeek R1 LLM.

grade: devel
confinement: strict

compression: lzo

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc

plugs:
  intel-npu:
    interface: custom-device
    custom-device: intel-npu-device
  npu-libs:
    interface: content
    content: npu-libs-2404
    target: $SNAP/npu-libs
  # To allow sideloading models by root
  # Also, it works around https://github.com/canonical/deepseek-r1-snap/issues/23
  home:
    read: all
    
hooks:
  install:
    environment: &install-env
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
    plugs: &install-plugs
      # For hardware-info
      - hardware-observe
      - opengl

parts:
  app-scripts:
    source: apps
    plugin: dump
    organize:
      "*": bin/

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/

  ml-snap-utils:
    source: ml-snap-utils
    plugin: go
    build-snaps:
      - go/1.24/stable
    stage-packages:
      - pciutils
      - nvidia-utils # for Nvidia vRAM and Cuda Capability detection
      - clinfo # for Intel GPU vRAM detection

  go-chat-client:
    source: https://github.com/jpm-canonical/go-chat-client.git
    source-type: git
    source-tag: v1.0.0-beta
    plugin: go
    build-snaps:
      - go/1.24/stable

  common-runtime-dependencies:
    plugin: nil
    stage-packages:
      - wget # model init
      - jq # install hook
    stage-snaps:
      - yq # install hook

  llamacpp: &llamacpp-part
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b5794
    source-depth: 1
    plugin: cmake
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  llamacpp-legacy:
    <<: *llamacpp-part
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
      - -DGGML_F16C=OFF
      - -DGGML_FMA=OFF
      - -DGGML_BMI2=OFF
      - -DGGML_AVX2=OFF
    organize:
      "*": (component/llamacpp-legacy)

  llamacpp-avx512:
    <<: *llamacpp-part
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
      - -DGGML_AVX512=ON
    organize:
      "*": (component/llamacpp-avx512)

  llamacpp-cuda:
    <<: *llamacpp-part
    build-packages:
      - to amd64: # see also the overrides
        - nvidia-cuda-toolkit
    # stage-packages:
    #   - libgomp1
    #   - libcudart12
    #   - libcublas12
    cmake-parameters:
      - -DLLAMA_CURL=OFF
      - -DGGML_NATIVE=OFF
      - -DGGML_CUDA=ON
      - -DCMAKE_CUDA_ARCHITECTURES=all-major
    override-pull: &amd64-only |
      if [ "$CRAFT_ARCH_BUILD_FOR" == "amd64" ]; then
        craftctl default
      fi
    override-build: *amd64-only
    override-stage: *amd64-only
    override-prime: *amd64-only
    organize:
      "*": (component/llamacpp-cuda)

  llama-aio: &llama-aio-part
    source: https://github.com/AmpereComputingAI/llama.cpp/releases/download/v3.1.0/llama_aio_v3.1.0_d1804d6.tar.gz
    source-type: tar
    plugin: dump
    stage-packages:
      - libcurl4
    organize:
      "LICENSE*.txt": usr/share/doc/llama-aio/
      llama-server: (component/llama-aio)/bin/
      libllama.so: (component/llama-aio)/lib/
      "libggml*.so": (component/llama-aio)/lib/
      "usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/": (component/llama-aio)/lib/
    prime:
      - -*

  llama-aio-ampereone:
    <<: *llama-aio-part
    source: https://github.com/AmpereComputingAI/llama.cpp/releases/download/v3.1.0/llama_aio_v3.1.0_d1804d6_ampereone.tar.gz
    organize:
      "LICENSE*.txt": usr/share/doc/llama-aio-ampereone/
      llama-server: (component/llama-aio-ampereone)/bin/
      libllama.so: (component/llama-aio-ampereone)/lib/
      "libggml*.so": (component/llama-aio-ampereone)/lib/
      "usr/lib/$CRAFT_ARCH_TRIPLET_BUILD_FOR/": (component/llama-aio-ampereone)/lib/

  model-distill-qwen-1-5b-q8-0-gguf:
    source: components/model-distill-qwen-1-5b-q8-0-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-qwen-1-5b-q8-0-gguf)

  model-distill-qwen-7b-q4-k-m-gguf:
    source: components/model-distill-qwen-7b-q4-k-m-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-qwen-7b-q4-k-m-gguf)

  model-distill-llama-70b-q4-k-m-gguf:
    source: components/model-distill-llama-70b-q4-k-m-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-llama-70b-q4-k-m-gguf)

  model-distill-qwen-7b-openvino-int4:
    source: components/model-distill-qwen-7b-openvino-int4
    plugin: dump
    override-build: |
      # At startup OVMS (single model mode) creates the mediapipe graph file inside the model directory.
      # This directory is read-only when it is inside a component.
      # By adding a symlink with the file's name inside the model directory, pointing to /tmp,
      # OVMS will follow the symlink during runtime and instead create the file in /tmp.
      mediapipe_graph_path="DeepSeek-R1-Distill-Qwen-7B-ov-int4/graph.pbtxt"
      if [ ! -L "$mediapipe_graph_path" ]; then
        ln -s /tmp/graph.pbtxt "$mediapipe_graph_path"
      fi

      craftctl default
    organize:
      "*": (component/model-distill-qwen-7b-openvino-int4)

  # Source: https://github.com/canonical/openvino-ai-plugins-gimp-snap/blob/be8968938844436e48a1a94643c8e95b43062653/snap/snapcraft.yaml#L119-L150
  opencl-driver:
    # Includes all the compute runtime and OpenCL bits needed for Intel GPU support
    plugin: nil
    build-packages:
      - wget
    override-build: |
      mkdir -p neo
      cd neo
      # Install Intel graphics compiler and compute runtime
      # This is required to enable GPU support for OpenVINO
      # https://docs.openvino.ai/2024/get-started/configurations/configurations-intel-gpu.html
      wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-core-2_2.5.6+18417_amd64.deb
      wget https://github.com/intel/intel-graphics-compiler/releases/download/v2.5.6/intel-igc-opencl-2_2.5.6+18417_amd64.deb
      wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-level-zero-gpu_1.6.32224.5_amd64.deb
      wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/intel-opencl-icd_24.52.32224.5_amd64.deb
      wget https://github.com/intel/compute-runtime/releases/download/24.52.32224.5/libigdgmm12_22.5.5_amd64.deb
      dpkg --root=$CRAFT_PART_INSTALL --force-all -i *.deb
      # update paths to the Intel Installable Client Drivers (ICDs) for OpenCL
      intel_icd="${CRAFT_PART_INSTALL}"/etc/OpenCL/vendors/intel.icd
      intel_icd_so_path=$(cat ${intel_icd})
      base_path="/snap/${SNAPCRAFT_PROJECT_NAME}/current"
      echo "${base_path}""${intel_icd_so_path}" > "${intel_icd}"
      intel_legacy1_icd="${CRAFT_PART_INSTALL}"/etc/OpenCL/vendors/intel_legacy1.icd
      if [ -f ${intel_legacy1_icd} ]; then
        intel_legacy1_icd_so_path=$(cat ${intel_legacy1_icd})
        echo "${base_path}""${intel_legacy1_icd_so_path}" > "${intel_legacy1_icd}"
      fi
      # fix broken sym links
      cd "${CRAFT_PART_INSTALL}"
      ln -sf "${base_path}"/usr/bin/ocloc-24.39.1 etc/alternatives/ocloc
      ln -sf "${base_path}"/etc/alternatives/ocloc usr/bin/ocloc
      craftctl default

  openvino-model-server:
    plugin: dump
    source: https://github.com/openvinotoolkit/model_server/releases/download/v2025.2/ovms_ubuntu24_python_on.tar.gz
    organize:
      "*": (component/openvino-model-server)
    stage-packages:
      - libxml2
      - curl
      - libpython3.12

  openvino-model-server-python-dependencies:
    plugin: python
    source: components/openvino-model-server
    python-packages:
     - Jinja2==3.1.6
     - MarkupSafe==3.0.2
    organize:
      "*": (component/openvino-model-server)

components:
  #
  # Engines
  #

  llamacpp:
    type: standard
    summary: llama.cpp Engine using default CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llamacpp-legacy:
    type: standard
    summary: llama.cpp Engine using only SSE4.2 and AVX CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llamacpp-avx512:
    type: standard
    summary: llama.cpp Engine using default and AVX512 CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llamacpp-cuda:
    type: standard
    summary: llama.cpp Engine for NVIDIA GPUs using CUDA
    description: LLM inference in C/C++
    version: *llamacpp-tag

  llama-aio:
    type: standard
    summary: llama-aio Engine for Ampere Altra CPUs
    description: LLM inference in C/C++
    version: v3.1.0

  llama-aio-ampereone:
    type: standard
    summary: llama-aio Engine for Ampere One CPUs
    description: LLM inference in C/C++
    version: v3.1.0

  openvino-model-server:
    type: standard
    summary: OpenVINO Model Server
    description: OpenVINO Model Server for serving models
    version: 2025.2

  # 
  # Models
  #

  model-distill-qwen-1-5b-q8-0-gguf:
    type: standard
    summary: DeepSeek R1 Distill Qwen 1.5B Q8
    description: Quantized model with 1.5B parameters in gguf format with Q8_0 weight encoding
    version: *snap-version

  model-distill-qwen-7b-q4-k-m-gguf:
    type: standard
    summary: DeepSeek R1 Distill Qwen 7B Q4
    description: Quantized model with 7B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

  model-distill-llama-70b-q4-k-m-gguf:
    type: standard
    summary: DeepSeek R1 Llama 70B Q4
    description: Quantized model with 70B parameters in gguf format with Q4_K_M weight encoding
    version: *snap-version

  model-distill-qwen-7b-openvino-int4:
    type: standard
    summary: DeepSeek R1 Qwen 7B GPU OpenVINO
    description: Model with 7B parameters in OpenVINO format
    version: *snap-version

apps:
  deepseek-r1:
    command: bin/stack
    plugs:
      # For hardware detection
      - opengl
      - hardware-observe
      - intel-npu 
      - npu-libs  
      # To download resources
      - network
    environment:
      # To load libigdrcl.so, for Intel GPU vRAM
      OCL_ICD_VENDORS: $SNAP/etc/OpenCL/vendors
      CHAT: $SNAP/bin/chat.sh
    
  server:
    command: bin/server.sh
    daemon: simple
    install-mode: disable
    plugs:
      # For inference and hardware detection (via init)
      - opengl
      # Needed by for cuda-uvmfd to listen on a unix socket?
      # Needed for server app by inheritance
      - network-bind
      # Needed to download resources
      - network
      # For hardware detection (via init)
      - hardware-observe
      # For sideloading models
      - home 
      # Intel NPU access (device node)
      - intel-npu 
      # Intel NPU access (libs) 
      - npu-libs  
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR
