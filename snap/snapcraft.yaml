name: mistral-7b-instruct
base: core24
version: &snap-version "v0.3"
summary: Mistral 7B Instruct
description: |
  This snap installs an optimized environment for inference with the
  mistral-7b-instruct LLM.


grade: devel
confinement: strict

compression: lzo

plugs:
  graphics:
    interface: content
    target: $SNAP/graphics

environment:
  # Workaround until it gets added to snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

parts:
  local:
    source: snap/local
    plugin: dump

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/
  
  mistral-inference:
    source: snap/local/.static
    source-type: file
    plugin: python
    python-packages:
      - mistral-inference==1.5.0
    organize:
      "*": (component/mistral-inference)

  llamacpp:
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b4130 
    source-depth: 1
    plugin: make
    override-build: |
      make --jobs=$(nproc)
      cp -v llama-cli $CRAFT_PART_INSTALL/
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  model:
    source: models/mistral-7B-Instruct-v0.3.tar
    plugin: dump
    organize:
      "*": (component/model)
    override-build: |
      craftctl default
      chmod 664 $CRAFT_PART_INSTALL/consolidated.safetensors

  model-q4-k-m-gguf:
    source: models/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
    source-type: file
    plugin: dump
    organize:
      "*": (component/model-q4-k-m-gguf)

components:
  # engines
  mistral-inference:
    type: test
    summary: Mistral Inference
    description: Inference Engine from Mistral
    version: "1.5.0" # should match the python package
  llamacpp:
    type: test
    summary: llama.cpp Engine
    description: LLM inference in C/C++
    version: *llamacpp-tag
  
  # models
  model:
    type: test
    summary: Mistral 7B Instruct
    description: Original model with safetensors, params, and tokenizer
    version: *snap-version
  model-q4-k-m-gguf:
    type: test
    summary: Mistral 7B Instruct v0.3 Q4_K_M
    description: Quantized model in gguf format with Q4_K_M weight encoding
    version: *snap-version

apps:
  chat:
    command: bin/chat.sh
    plugs:
      - opengl
      # Needed by mistral-inference,
      # for cuda-uvmfd to listen on a unix socket
      - network-bind
    environment:
      # Needed for shared libraries used by llama.cpp's cli
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR