name: mistral-7b-instruct
base: core24
version: "v0.3+0.0.1"
summary: Mistral 7B Instruct
description: Inference with Mistral 7B Instruct

grade: devel
confinement: devmode

compression: lzo

parts:
  mistral-inference:
    source: src/engine
    plugin: python
    python-packages:
      - mistral-inference
  
  mistral-cli:
    source: src/cli
    plugin: dump

  model-mistral-7b-instruct:
    # source: https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar
    # source-checksum: 80b71fcb6416085bcb4efad86dfb4d52
    source: mistral-7B-Instruct-v0.3.tar
    plugin: dump
    organize:
      consolidated.safetensors: (component/model-mistral-7b-instruct)/consolidated.safetensors
      params.json: (component/model-mistral-7b-instruct)/params.json
      tokenizer.model.v3: (component/model-mistral-7b-instruct)/tokenizer.model.v3
    override-build: |
      craftctl default
      chmod 664 $CRAFT_PART_INSTALL/consolidated.safetensors

components:
  model-mistral-7b-instruct:
    type: test
    summary: Mistral 7B Instruct
    description: Model with safetensors, params, and tokenizer
    version: "v0.3"

apps:
  prompt:
    command: prompt.py
    plugs:
      - opengl

plugs:
  graphics-core22:
    interface: content
    target: $SNAP/graphics
