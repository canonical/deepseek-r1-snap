name: deepseek-r1
base: core24
version: &snap-version "v3" # Refers to DeepSeek V3 base
summary: DeepSeek R1
description: |
  This snap installs an optimized environment for inference with the
  DeepSeek R1 LLM.

grade: devel
confinement: strict

compression: lzo

plugs:
  graphics:
    interface: content
    target: $SNAP/graphics

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc

hooks:
  install:
    plugs: &install-plugs
      # For hardware-info
      - hardware-observe
      - opengl

parts:
  init:
    source: init.sh
    source-type: file
    plugin: dump
  
  chat:
    source: chat.sh
    source-type: file
    plugin: dump

  server:
    source: server.sh
    source-type: file
    plugin: dump

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/

  ml-snap-utils:
    source: ml-snap-utils
    plugin: go
    build-snaps:
      - go/latest/stable
    stage-packages:
      - pciutils
      - nvidia-utils

  common-runtime-dependencies:
    plugin: nil
    stage-packages:
      - wget # model init
      - jq # install hook
    stage-snaps:
      - yq # install hook

  llamacpp: &llamacpp-part
    source: https://github.com/ggerganov/llama.cpp.git
    source-tag: &llamacpp-tag b4595 
    source-depth: 1
    plugin: cmake
    cmake-parameters:
      - -DGGML_NATIVE=OFF
      - -DGGML_AVX=ON
      - -DGGML_AVX2=OFF
    stage-packages:
      - libgomp1
    organize:
      # move everything, including the staged packages
      "*": (component/llamacpp)

  llamacpp-avx512:
    <<: *llamacpp-part
    cmake-parameters:
      - -DGGML_NATIVE=OFF
      - -DGGML_AVX=ON
      - -DGGML_AVX2=ON
      - -DGGML_AVX512=ON
    organize:
      "*": (component/llamacpp-avx512)

  model-distill-qwen-q8-gguf:
    source: components/model-distill-qwen-q8-gguf
    plugin: dump
    organize:
      "*": (component/model-distill-qwen-q8-gguf)
      
components:
  llamacpp:
    type: test
    summary: llama.cpp Engine
    description: LLM inference in C/C++
    version: *llamacpp-tag
  llamacpp-avx512:
    type: test
    summary: llama.cpp Engine using avx, avx2 and avx512 CPU instruction sets
    description: LLM inference in C/C++
    version: *llamacpp-tag
  
  # models
  model-distill-qwen-q8-gguf:
    type: test
    summary: DeepSeek R1 Distill Qwen 1.5B
    description: Quantized model in gguf format with Q8_0 weight encoding
    version: *snap-version

apps:
  # For testing purposes. Use sudo <snap>.re-install
  # Connecting mistral-7b-instruct:hardware-observe is required even if installed in dev mode!
  re-install:
    command: meta/hooks/install
    plugs: *install-plugs

  init:
    command: init.sh
    plugs:
      # Needed to download resources
      - network

  chat: &engine-app
    command: chat.sh
    plugs:
      - opengl
      # Needed by mistral-inference,
      # for cuda-uvmfd to listen on a unix socket
      - network-bind
      # Needed to download resources
      - network
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

  server: 
    <<: *engine-app
    command: server.sh
    daemon: simple
    install-mode: disable
