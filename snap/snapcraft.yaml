name: deepseek-r1
base: core24
version: &snap-version "v3" # Refers to DeepSeek V3 base
summary: DeepSeek R1
description: |
  This snap installs an optimized environment for inference with the
  DeepSeek R1 LLM.

grade: devel
confinement: strict

compression: lzo

environment:
  # Workaround until it gets set by snapd
  SNAP_COMPONENTS: /snap/$SNAP_INSTANCE_NAME/components/$SNAP_REVISION

layout:
  # Wget's default config file
  # This is to avoid runtime errors and AppArmor denials
  /etc/wgetrc:
    bind-file: $SNAP/etc/wgetrc

hooks:
  install:
    plugs: &install-plugs
      # For hardware-info
      - hardware-observe
      - opengl
  configure:
    plugs: *install-plugs

parts:
  app-scripts:
    source: apps
    plugin: dump
    organize:
      "*": bin/

  stacks:
    source: stacks
    plugin: dump
    organize:
      "*": stacks/

  ml-snap-utils:
    source: ml-snap-utils
    plugin: go
    build-snaps:
      - go/latest/stable
    stage-packages:
      - pciutils
      - nvidia-utils

  # common-runtime-dependencies:
  #   plugin: nil
  #   stage-packages:
  #     - wget # model init
  #     - jq # install hook
  #   stage-snaps:
  #     - yq # install hook

  # llamacpp: &llamacpp-part
  #   source: https://github.com/ggerganov/llama.cpp.git
  #   source-tag: &llamacpp-tag b4595 
  #   source-depth: 1
  #   plugin: cmake
  #   cmake-parameters:
  #     - -DGGML_NATIVE=OFF
  #   stage-packages:
  #     - libgomp1
  #   organize:
  #     # move everything, including the staged packages
  #     "*": (component/llamacpp)

  # llamacpp-avx512:
  #   <<: *llamacpp-part
  #   cmake-parameters:
  #     - -DGGML_NATIVE=OFF
  #     - -DGGML_AVX512=ON
  #   organize:
  #     "*": (component/llamacpp-avx512)

  # llamacpp-cuda:
  #   <<: *llamacpp-part
  #   build-packages:
  #     - nvidia-cuda-toolkit
  #   # stage-packages:
  #   #   - libgomp1
  #   #   - libcudart12
  #   #   - libcublas12
  #   cmake-parameters:
  #     - -DGGML_NATIVE=OFF
  #     - -DGGML_CUDA=ON
  #     - -DCMAKE_CUDA_ARCHITECTURES=all-major
  #   organize:
  #     "*": (component/llamacpp-cuda)

  # model-distill-qwen-1-5b-q8-0-gguf:
  #   source: components/model-distill-qwen-1-5b-q8-0-gguf
  #   plugin: dump
  #   organize:
  #     "*": (component/model-distill-qwen-1-5b-q8-0-gguf)
  
  # model-distill-qwen-7b-q4-k-m-gguf:
  #   source: components/model-distill-qwen-7b-q4-k-m-gguf
  #   plugin: dump
  #   organize:
  #     "*": (component/model-distill-qwen-7b-q4-k-m-gguf)
  
  # model-distill-llama-70b-q4-k-m-gguf:
  #   source: components/model-distill-llama-70b-q4-k-m-gguf
  #   plugin: dump
  #   organize:
  #     "*": (component/model-distill-llama-70b-q4-k-m-gguf)

  openvino-model-server:
    plugin: dump
    source: https://github.com/openvinotoolkit/model_server/releases/download/v2025.1/ovms_ubuntu24_python_on.tar.gz
    organize:
      "*": (component/openvino-model-server)
    stage-packages:
      - libxml2
      - curl
      - libpython3.12

  openvino-model-server-python-dependencies:
    plugin: python
    source: components/openvino-model-server
    python-packages: 
     - Jinja2==3.1.6
     - MarkupSafe==3.0.2
    organize:
      "*": (component/openvino-model-server)

  # model-distill-qwen-7b-gpu-openvino:
  #   source: components/model-distill-qwen-7b-gpu-openvino
  #   plugin: dump
  #   organize:
  #     "*": (component/model-distill-qwen-7b-gpu-openvino)
      
components:
  # engines
  # llamacpp:
  #   type: standard
  #   summary: llama.cpp Engine
  #   description: LLM inference in C/C++
  #   version: *llamacpp-tag
  # llamacpp-avx512:
  #   type: standard
  #   summary: llama.cpp Engine using avx, avx2 and avx512 CPU instruction sets
  #   description: LLM inference in C/C++
  #   version: *llamacpp-tag
  # llamacpp-cuda:
  #   type: standard
  #   summary: llama.cpp Engine for NVIDIA GPUs using CUDA
  #   description: LLM inference in C/C++
  #   version: *llamacpp-tag

  
  # models
  # model-distill-qwen-1-5b-q8-0-gguf:
  #   type: standard
  #   summary: DeepSeek R1 Distill Qwen 1.5B Q8
  #   description: Quantized model with 1.5B parameters in gguf format with Q8_0 weight encoding
  #   version: *snap-version

  # model-distill-qwen-7b-q4-k-m-gguf:
  #   type: standard
  #   summary: DeepSeek R1 Distill Qwen 7B Q4
  #   description: Quantized model with 7B parameters in gguf format with Q4_K_M weight encoding
  #   version: *snap-version

  # model-distill-llama-70b-q4-k-m-gguf:
  #   type: standard
  #   summary: DeepSeek R1 Llama 70B Q4
  #   description: Quantized model with 70B parameters in gguf format with Q4_K_M weight encoding
  #   version: *snap-version

  openvino-model-server:
    type: standard
    summary: OpenVINO Model Server
    description: OpenVINO Model Server for serving models
    version: 2025.1

  model-distill-qwen-7b-gpu-openvino:
    type: standard
    summary: DeepSeek R1 Qwen 7B GPU OpenVINO
    description: Model with 7B parameters in OpenVINO format
    version: *snap-version

apps:
  # For testing purposes. Use sudo <snap>.re-install
  # Connecting the hardware-observe interface is required even if installed in dev mode!
  re-install:
    command: meta/hooks/install
    plugs: *install-plugs

  init:
    command: bin/init.sh
    plugs:
      # Needed to download resources
      - network
      # For hardware detection
      - hardware-observe
      - opengl

  chat: &engine-app
    command: bin/chat.sh
    plugs:
      # For inference and hardware detection (via init)
      - opengl
      # Needed by for cuda-uvmfd to listen on a unix socket?
      # Needed for server app by inheritance
      - network-bind
      # Needed to download resources
      - network
      # For hardware detection (via init)
      - hardware-observe
      # For sideloading models
      - home 
    environment:
      # Needed for shared libraries used by llama.cpp
      # TODO: Should this instead be added in an env file to llamacpp component
      ARCH_TRIPLET: $CRAFT_ARCH_TRIPLET_BUILD_FOR

  server: 
    <<: *engine-app
    command: bin/server.sh
    daemon: simple
    install-mode: disable
